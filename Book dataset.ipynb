{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SdFZ6eWLKNo",
    "outputId": "45874fb6-47fb-498f-c0ef-89ca390f3910"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA model saved successfully to: /content/lda_model\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Step 1: Read the text from the file\n",
    "file_path = \"/content/chap.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Text Preprocessing\n",
    "# Tokenization : breaking down a text into smaller units, called tokens, which can be words, phrases\n",
    "# it is essential for understanding structure and meaning of the text\n",
    "#in NLP we know words act as features , tokenization enables the extraction of this feature\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords : The purpose of removing stopwords is to focus on the more informative words in a text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "\n",
    "# Lemmatization \n",
    "# Lemmatization is a linguistic and natural language processing (NLP) technique that involves reducing words to\n",
    "# their base or root form. \n",
    "# reduced set of features can improve model performance.\n",
    "# Lemmatization reduces the vocabulary size by grouping inflected forms into a single lemma.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Step 3: Corpus Generation (assuming each document is a single line in the text file)\n",
    "documents = [tokens]  # In case each line represents a document, otherwise, split the text into documents\n",
    "\n",
    "# Create dictionary and corpus\n",
    "# In linguistics and natural language processing (NLP), a corpus \n",
    "# refers to a collection of written or spoken texts that are used as a basis for linguistic analysis, language modeling, and various computational tasks. Corpora serve as large, structured data sources for studying language patterns, understanding linguistic phenomena, and building language models and algorithms.\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Step 4: Train LDA model\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA) is a probabilistic model and a \n",
    "# popular technique in natural language processing (NLP) and machine learning for topic modeling.\n",
    "# LDA assumes that documents are mixtures of topics, and each topic is a distribution over words.\n",
    "# A document is seen as a mixture of topics, where each word in the document is associated with a particular topic.\n",
    "\n",
    "num_topics = 5\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "# Step 5: Save the trained model to a file\n",
    "model_path = \"/content/lda_model\"\n",
    "lda_model.save(model_path)\n",
    "print(\"Trained LDA model saved successfully to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUuTvmozLrAf",
    "outputId": "516af087-0258-4bb0-db25-a6e410029c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Topics: [(1, 0.96016765), (2, 0.036938675)]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "# the use of gensim : \n",
    "# Gensim is an open-source Python library designed for natural language processing (NLP) and topic modeling. \n",
    "# It is particularly known for its implementations of various algorithms for semantic analysis and document similarity analysis.\n",
    "# Step 1: Read the text from the file\n",
    "file_path = \"/content/chap.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Preprocess the Text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and non-alphanumeric words, convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Step 3: Create Dictionary and Corpus\n",
    "def create_corpus(text):\n",
    "    # Create a Gensim dictionary from the preprocessed text\n",
    "    dictionary = corpora.Dictionary([text])\n",
    "    \n",
    "    # Create a Gensim corpus using the dictionary\n",
    "    corpus = [dictionary.doc2bow(text)]\n",
    "    \n",
    "    return dictionary, corpus\n",
    "\n",
    "# Step 4: Infer Topic Distribution\n",
    "def infer_topic_distribution(lda_model, corpus):\n",
    "    # Infer the topic distribution of the corpus using the pre-trained LDA model\n",
    "    return lda_model.get_document_topics(corpus[0])\n",
    "\n",
    "# Step 5: Assign Topics to Text\n",
    "def assign_topics_to_text(topic_distribution, num_topics):\n",
    "    # Sort the topics by probability and return the top 'num_topics' topics\n",
    "    sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_topics[:num_topics]\n",
    "\n",
    "# Preprocess the text\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "# Load pre-trained LDA model\n",
    "lda_model = models.LdaModel.load('/content/lda_model')\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary, corpus = create_corpus(processed_text)\n",
    "\n",
    "# Infer topic distribution\n",
    "\n",
    "topic_distribution = infer_topic_distribution(lda_model, corpus)\n",
    "\n",
    "# Assign topics to text\n",
    "num_topics = 5  # Specify the number of topics to assign\n",
    "assigned_topics = assign_topics_to_text(topic_distribution, num_topics)\n",
    "\n",
    "# Print the assigned topics\n",
    "print(\"Assigned Topics:\", assigned_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWA5DOsU16oq",
    "outputId": "67009388-9bb6-40c0-ee71-a776cbcac6f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# # nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YzqJU3lLgNj",
    "outputId": "bd3b8e54-62fe-4047-fd77-f205b350ea74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the model: __randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/content/lda_model', 'rb') as f:\n",
    "    try:\n",
    "        pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Q1pe05vL0E-",
    "outputId": "03dee14b-24f7-4e6e-9e3d-13a54a727b3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Topics: [(1, 0.9991922)]\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary, corpus = create_corpus(processed_text)\n",
    "\n",
    "# Infer topic distribution\n",
    "topic_distribution = infer_topic_distribution(lda_model, corpus)\n",
    "\n",
    "# Assign topics to text\n",
    "assigned_topics = assign_topics_to_text(topic_distribution, num_topics)\n",
    "\n",
    "print(\"Assigned Topics:\", assigned_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qh2R4tlMHc-",
    "outputId": "13bb3ef3-600c-41fd-ea01-de8bb569383e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:\n",
      "Original Text: social\n",
      "Assigned Topics: (1, 0.99960583)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary, corpus = create_corpus(processed_text)\n",
    "\n",
    "# Infer topic distribution\n",
    "topic_distribution = infer_topic_distribution(lda_model, corpus)\n",
    "\n",
    "# Assign topics to text\n",
    "assigned_topics = assign_topics_to_text(topic_distribution, num_topics)\n",
    "\n",
    "# Print assigned topics and original text\n",
    "for i, (text, topics) in enumerate(zip(processed_text, assigned_topics)):\n",
    "    print(f\"Text {i+1}:\")\n",
    "    print(\"Original Text:\", text)\n",
    "    print(\"Assigned Topics:\", topics)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPIST1KgM_WH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
